{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Fine-Tune a Generative AI Model for News Article Summarization </center>","metadata":{}},{"cell_type":"markdown","source":"In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [Zephy-7b](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{}},{"cell_type":"markdown","source":"- [ 1 - Set up - Load Required Dependencies, Dataset and LLM](#1)\n  - [ 1.1 - Set up Required Dependencies](#1.1)\n  - [ 1.2 - Load Dataset and LLM](#1.2)\n  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n- [ 2 - Perform Full Fine-Tuning](#2)\n  - [ 2.1 - Preprocess the news-Summary Dataset](#2.1)\n  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n  - [ 3.2 - Train PEFT Adapter](#3.2)\n  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)","metadata":{}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Set up - Load Required Dependencies, Dataset and LLM","metadata":{}},{"cell_type":"markdown","source":"<a name='1.1'></a>\n### 1.1 - Set up Required Dependencies","metadata":{}},{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install -U transformers\n%pip install -U datasets \n%pip install evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:10:24.934514Z","iopub.execute_input":"2023-12-27T14:10:24.935259Z","iopub.status.idle":"2023-12-27T14:11:32.433280Z","shell.execute_reply.started":"2023-12-27T14:10:24.935219Z","shell.execute_reply":"2023-12-27T14:11:32.431679Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.2)\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.16.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Import the necessary components.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-27T14:11:32.436024Z","iopub.execute_input":"2023-12-27T14:11:32.436895Z","iopub.status.idle":"2023-12-27T14:11:39.985675Z","shell.execute_reply.started":"2023-12-27T14:11:32.436825Z","shell.execute_reply":"2023-12-27T14:11:39.984648Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb-key\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\npersonal_key_for_api = user_secrets.get_secret()\n\n! wandb login $personal_key_for_api","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='1.2'></a>\n### 1.2 - Load Dataset and LLM\n\nYou are going to continue experimenting with the [ News-Sum](https://huggingface.co/datasets/glnmario/news-qa-summarization) Hugging Face dataset. It contains 10,000+ news article with the corresponding manually labeled summaries and question answers. ","metadata":{}},{"cell_type":"code","source":"huggingface_dataset_name = \"glnmario/news-qa-summarization\"\n\ndataset = (load_dataset(huggingface_dataset_name, data_files=\"data.jsonl\", split='train').train_test_split(train_size=800, test_size=200))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:11:58.755632Z","iopub.execute_input":"2023-12-27T14:11:58.756070Z","iopub.status.idle":"2023-12-27T14:11:59.223223Z","shell.execute_reply.started":"2023-12-27T14:11:58.756035Z","shell.execute_reply":"2023-12-27T14:11:59.222103Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Load the pre-trained [ZEPHYR-7b model](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) from HuggingFace. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model.","metadata":{}},{"cell_type":"code","source":"\n# model_name='HuggingFaceH4/zephyr-7b-beta'\n# model = AutoModelForCausalLM.from_pretrained('HuggingFaceH4/zephyr-7b-beta', torch_dtype=torch.bfloat16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that to understand the differences of full fine-tuning and LoRA methods.","metadata":{}},{"cell_type":"code","source":"# def print_number_of_trainable_model_parameters(model):\n#     trainable_model_params = 0\n#     all_model_params = 0\n#     for _, param in model.named_parameters():\n#         all_model_params += param.numel()\n#         if param.requires_grad:\n#             trainable_model_params += param.numel()\n#     return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\n# print(print_number_of_trainable_model_parameters(model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"using low memory model","metadata":{}},{"cell_type":"code","source":"model_name='google/flan-t5-base'\n\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:12:05.707014Z","iopub.execute_input":"2023-12-27T14:12:05.707435Z","iopub.status.idle":"2023-12-27T14:12:10.751935Z","shell.execute_reply.started":"2023-12-27T14:12:05.707387Z","shell.execute_reply":"2023-12-27T14:12:10.750681Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"trainable model parameters: 247577856\nall model parameters: 247577856\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='1.3'></a>\n### 1.3 - Test the Model with Zero Shot Inferencing\n\nTest the model with the zero shot inferencing. You can see that the model struggles to news article the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand.","metadata":{}},{"cell_type":"code","source":"index = 40\n\nstory = dataset['train'][index]['story']\nsummary = dataset['train'][index]['summary']\nprompt =  f\"\"\"\nSummarize the following article.\n\n{story}\n\nSummary:\n\"\"\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    original_model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=60,\n    )[0], \n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:12:15.343505Z","iopub.execute_input":"2023-12-27T14:12:15.343964Z","iopub.status.idle":"2023-12-27T14:12:18.876133Z","shell.execute_reply.started":"2023-12-27T14:12:15.343930Z","shell.execute_reply":"2023-12-27T14:12:18.875026Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (936 > 512). Running this sequence through the model will result in indexing errors\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following article.\n\nWASHINGTON (CNN)  -- President Richard M. Nixon and his Brazilian counterpart, Emilio Medici, in 1971 discussed ways their countries could work together to overthrow the socialist government of Salvador Allende in Chile, according to a newly declassified document. President Richard M. Nixon, right, and his Brazilian counterpart, Emilio Medici. During a meeting of the two leaders at the White House on December 9 of that year, Medici was discussing the possibility of a coup by the Chilean military with assistance from Brazilian military officers when Nixon said that it was \"very important that Brazil and the United States work closely in this field,\" according to the document. Nixon offered money or other discreet aid for the effort if it could be made available, the document shows. \"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\" Nixon said. Medici said he was \"happy to see that the Brazilian and American positions and views\" were so close. The declassified document, a previously top secret memorandum for Nixon's file written by National Security Adviser Henry Kissinger, was published by the National Security Archive, a nongovernmental research institute in Washington. The memorandum, along with other documents, were declassified in July as part of the State Department's Foreign Relations of the United States series. \"This is an explosive document that details collusion between the colossus of the North [the United States] and the colossus of the South [Brazil],\" said Peter Kornbluh, the director of a Chile and Brazil Documentation Project for the National Security Archive. He called it \"a smoking gun of confirmation of Brazil's effort to engage in operations to overthrow the government of Chile and a discussion of collusion with the United States.\" The two leaders also discussed the creation of a back channel for direct communication outside normal diplomatic protocols, according to the document. Each designated personal aides to carry handwritten communications back and forth to keep discussions out of official records. \"I think there is precedent, but we've never seen it detailed in a document this way, in which two presidents set up the utmost secret of back-channel communications so they can discuss the most sensitive aspects of collusion and collaboration in efforts to challenge the left in Latin America and change the futures of select Latin American governments,\" Kornbluh said. He added that \"there's a significant paper trail of evidence of what that collusion was that remains secret, and we're going to have to press Brazil and Washington to recover those documents.\" Despite the leaders' effort to keep the subject of their talks secret, word got out. A declassified CIA memorandum written some time after the Nixon-Medici meeting in Washington said that word of the secret talks between the two leaders about shaping Brazilian foreign policy filtered down to Brazilian military officers by a \"Cabinet leak.\" Gen. Vicente Dale Coutinho, commander of Brazil's 4th Army, reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\" it said. A declassified CIA national intelligence estimate written in 1972 concluded, \"Brazil will be playing a bigger role in hemispheric affairs and seeking to fill whatever vacuum the U.S. leaves behind. It is unlikely that Brazil will intervene openly in its neighbors' internal affairs, but the regime will not be above using the threat of intervention or tools of diplomacy and covert action to oppose leftist regimes, to keep friendly governments in office, or to help place them there.\" The newly published documents do not offer any conclusive proof of Brazilian involvement in the Chilean coup of 1973, which the Nixon administration supported. Kornbluh said that with the passage of time and change in governments in all of the countries involved, the real story of diplomatic and covert collusion between the United States and Brazil in Chile can finally be told. The National Security Archive will push for the declassification and publication of more top-secret documents from the Nixon library, he said, and will approach Brazilian government officials to ask for their cooperation in getting documents released in Washington and Brasilia, the capital of\n\nSummary:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\nDocument shows Richard M. Nixon, Emilio Medici had like-minded goals .\nIt says two leaders met at White House, Nixon offered money or other discreet aid .\nCIA memo says Brazilian general thought U.S. wanted Brazil \"to do the dirty work\"\nMemo, other documents were declassified in July of this year as part of project .\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nThe declassified document, a previously top secret memorandum for Nixon's file, was published by the National Security Archive.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='2'></a>\n## 2 - Perform Full Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"<a name='2.1'></a>\n### 2.1 - Preprocess the News-Summary Dataset\n\nYou need to convert the News-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following article` and to the start of the summary with `Summary` as follows:\n\nTraining prompt (dialogue):\n```\nSummarize the following article.\n\nFor many travelers, duty-free is a luxurious enigma wrapped up in discounted Swiss chocolate and soaked in tax-free vodka. Duty-free goods are mostly sold inside international airport terminals, ferry stations, cruise ports, and border stops. \n\nDuty-free shops sell products without local import tax. \n\nAs the name implies, duty-free shops sell products without duty (a.k.a. local import tax). For example, by buying goods in a duty-free shop at Paris's Charles de Gaulle, you avoid paying the duty that France slaps on imported goods (like Swedish vodka) and that French stores ordinarily include as part of a product's list price. \n\nIn Europe, there's a bonus perk: Duty-free shops in airports and ports are \"tax-free shops,\" too, which means you are spared the value added tax (or V.A.T., a type of sales tax) that would otherwise be included in the price of goods sold elsewhere in the European Union. That means a savings of between 5 and 25 percent, depending on the country. \n\nBut there's a catch for duty-free products bought in Europe and elsewhere. If you bring into the U.S. more than $800 worth of items purchased abroad -- duty-free or not -- you'll have to pay the U.S. duty. As a rule of thumb, Americans returning from overseas trips must pay 3 percent on the first $1,000 worth of merchandise over the $800 allowance. Import products worth even more than that and you may be taxed at a higher percentage. \n\nIn short, duty-free is hit-or-miss for Americans. The best deals are on items labeled \"tax free\" and otherwise taxed heavily -- such as alcohol and cigarettes. You may also find it worthwhile to shop in duty-free stores if you have some local currency left and would rather put it to use than redeem it for dollars (and get hit with the high conversion fee of a bank or currency exchange bureau). \n\nNot every duty-free item is a true bargain. Yngve Bia, president of the duty-free research company Generation Research, says price differences depend on two things: geography and currency exchange rates. \"Right now, Heathrow and Gatwick in London offer good deals, especially for liquor, because of the weak British pound,\" he says. For example, a one-liter bottle of Absolut vodka has a typical non-duty-free price of about $30 at retail U.S. shops. But travelers can buy it for just $15 (£10) at duty free shops at London's Heathrow and Gatwick airports. That's a significant savings.\n```\n    \nSummary: \n```\n\nTraining response (summary):\n```\nDuty-free shopping is hit-or-miss for Americans .\nThe best deals are on items labeled \"tax free\" and otherwise taxed heavily .\nFor countries in the EU, duty-free shops at airports and ports are also tax-free .\n```\n\nThen preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token).","metadata":{}},{"cell_type":"code","source":"def tokenize_function(example):\n    \n    start_prompt = 'Summarize the following article.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompt = [start_prompt + story + end_prompt for story in example[\"story\"]]\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    return example\n\n# The dataset actually contains 3 diff splits: train, validation, test.\n# The tokenize_function code is handling all data across all splits in batches.\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['story', 'questions', 'answers', 'summary',])","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:12:26.561972Z","iopub.execute_input":"2023-12-27T14:12:26.563053Z","iopub.status.idle":"2023-12-27T14:12:28.277446Z","shell.execute_reply.started":"2023-12-27T14:12:26.563013Z","shell.execute_reply":"2023-12-27T14:12:28.276476Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746d8df814194518bf41925bf2a25a3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8740a64231494bf899c763b98cc1c4a3"}},"metadata":{}}]},{"cell_type":"markdown","source":"To save some time in the lab, you will subsample the dataset:","metadata":{}},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 2 == 0, with_indices=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:12:33.344482Z","iopub.execute_input":"2023-12-27T14:12:33.345524Z","iopub.status.idle":"2023-12-27T14:12:34.245233Z","shell.execute_reply.started":"2023-12-27T14:12:33.345477Z","shell.execute_reply":"2023-12-27T14:12:34.244087Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ed06817a4684bb38610eef95ff9ce9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76711b134fce4b619705c1b06f5805d3"}},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")\n\nprint(tokenized_datasets)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:12:36.071874Z","iopub.execute_input":"2023-12-27T14:12:36.072288Z","iopub.status.idle":"2023-12-27T14:12:36.078436Z","shell.execute_reply.started":"2023-12-27T14:12:36.072258Z","shell.execute_reply":"2023-12-27T14:12:36.077391Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (400, 2)\nTest: (100, 2)\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 400\n    })\n    test: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 100\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='2.2'></a>\n### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n\nNow utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.","metadata":{}},{"cell_type":"code","source":"output_dir = f'./article-summary-training-{str(int(time.time()))}'\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-5,\n    num_train_epochs=15,\n    weight_decay=0.01,\n    logging_steps=1,\n    max_steps=1,\n    save_strategy='epoch'\n)\n\ntrainer = Trainer(\n    model=original_model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.train()\n#this will throw out of memory error due to memory constraints","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As this needs more memory and we could see the issue, Lets try the PeFT method","metadata":{}},{"cell_type":"markdown","source":"<a name='3'></a>\n## 3 - Perform Parameter Efficient Fine-Tuning (PEFT)\n\nNow, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon. \n\nPEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n\nThat said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning\n\nYou need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=16, # Rank\n    lora_alpha=16,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"all\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:20:25.314043Z","iopub.execute_input":"2023-12-27T14:20:25.314496Z","iopub.status.idle":"2023-12-27T14:20:25.322525Z","shell.execute_reply.started":"2023-12-27T14:20:25.314458Z","shell.execute_reply":"2023-12-27T14:20:25.321228Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"peft_model = get_peft_model(original_model, \n                            lora_config)\nprint(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:20:26.804474Z","iopub.execute_input":"2023-12-27T14:20:26.804967Z","iopub.status.idle":"2023-12-27T14:20:26.902768Z","shell.execute_reply.started":"2023-12-27T14:20:26.804927Z","shell.execute_reply":"2023-12-27T14:20:26.901537Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"trainable model parameters: 1770240\nall model parameters: 249347328\npercentage of trainable model parameters: 0.71%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Train PEFT Adapter\n\nDefine training arguments and create `Trainer` instance.","metadata":{}},{"cell_type":"code","source":"output_dir = f'./peft-article-summary-training-{str(int(time.time()))}'\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    auto_find_batch_size=True,\n    save_strategy='epoch',\n    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n    num_train_epochs=15,\n    logging_steps=1,\n    max_steps=20,\n    logging_dir=f'{output_dir}/logs'\n    \n)\n    \npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:20:33.351422Z","iopub.execute_input":"2023-12-27T14:20:33.352166Z","iopub.status.idle":"2023-12-27T14:20:33.371432Z","shell.execute_reply.started":"2023-12-27T14:20:33.352126Z","shell.execute_reply":"2023-12-27T14:20:33.370152Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\npeft_trainer.train()\n\npeft_model_path=\"./peft-article-summary-checkpoint-local\"\n\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:20:42.292677Z","iopub.execute_input":"2023-12-27T14:20:42.293425Z","iopub.status.idle":"2023-12-27T14:21:33.206047Z","shell.execute_reply.started":"2023-12-27T14:20:42.293386Z","shell.execute_reply":"2023-12-27T14:21:33.205023Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 00:47, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>43.500000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>47.500000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>44.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>43.750000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>43.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>40.500000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>38.750000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>37.500000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>36.000000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>34.250000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>33.500000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>32.500000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>32.250000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>30.375000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>29.250000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>29.625000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>28.875000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>28.625000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>28.500000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>28.125000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('./peft-article-summary-checkpoint-local/tokenizer_config.json',\n './peft-article-summary-checkpoint-local/special_tokens_map.json',\n './peft-article-summary-checkpoint-local/spiece.model',\n './peft-article-summary-checkpoint-local/added_tokens.json',\n './peft-article-summary-checkpoint-local/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n\npeft_model = PeftModel.from_pretrained(peft_model_base, \n                                       './peft-article-summary-checkpoint-local/', \n                                       torch_dtype=torch.bfloat16,\n                                       is_trainable=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:21:39.051892Z","iopub.execute_input":"2023-12-27T14:21:39.052335Z","iopub.status.idle":"2023-12-27T14:21:42.462259Z","shell.execute_reply.started":"2023-12-27T14:21:39.052302Z","shell.execute_reply":"2023-12-27T14:21:42.460939Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"The number of trainable parameters will be `0` due to `is_trainable=False` setting:","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:15:28.998683Z","iopub.execute_input":"2023-12-27T14:15:28.999614Z","iopub.status.idle":"2023-12-27T14:15:29.014157Z","shell.execute_reply.started":"2023-12-27T14:15:28.999575Z","shell.execute_reply":"2023-12-27T14:15:29.012993Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"trainable model parameters: 0\nall model parameters: 249347328\npercentage of trainable model parameters: 0.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3.3'></a>\n### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nMake inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model.","metadata":{}},{"cell_type":"code","source":"index = 10\nstory = dataset['test'][index]['story']\nbaseline_human_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following article.\n\n{story}\n\nSummary: \"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n# instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n# instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\npeft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\npeft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\n# print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n# print(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:15:31.333555Z","iopub.execute_input":"2023-12-27T14:15:31.333987Z","iopub.status.idle":"2023-12-27T14:15:41.132483Z","shell.execute_reply.started":"2023-12-27T14:15:31.333952Z","shell.execute_reply":"2023-12-27T14:15:41.131063Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (952 > 512). Running this sequence through the model will result in indexing errors\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\nJohn Stremlau: 20 years ago today, Nelson Mandela was released from Cape Town jail .\nStremlau: After 27 years, Mandela emerged \"without bitterness, his humanity intact\"\nMandela maintains commitment to democracy, freedom and rule of law, he writes .\nMandela is inspiration in troubled nation that desperately needs it, Stremlau says .\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\nJohn Stremlau: Mandela has declined to participate in this week's many celebrations in his honor.\n---------------------------------------------------------------------------------------------------\nPEFT MODEL: John Stremlau: The world's most famous political prisoner emerged without bitterness, his humanity intact.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3.4'></a>\n### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time).","metadata":{}},{"cell_type":"code","source":"stories = dataset['test'][0:10]['story']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\n# instruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, story in enumerate(stories):\n    prompt = f\"\"\"\nSummarize the following article.\n\n{story}\n\nSummary: \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    human_baseline_text_output = human_baseline_summaries[idx]\n    \n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n#     instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n#     instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\n    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n    original_model_summaries.append(original_model_text_output)\n#     instruct_model_summaries.append(instruct_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf.to_csv('model_output_results.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:21:47.560908Z","iopub.execute_input":"2023-12-27T14:21:47.561728Z","iopub.status.idle":"2023-12-27T14:23:16.028353Z","shell.execute_reply.started":"2023-12-27T14:21:47.561690Z","shell.execute_reply":"2023-12-27T14:23:16.027043Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Compute ROUGE score for this subset of the data. ","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\n# instruct_model_results = rouge.compute(\n#     predictions=instruct_model_summaries,\n#     references=human_baseline_summaries[0:len(instruct_model_summaries)],\n#     use_aggregator=True,\n#     use_stemmer=True,\n# )\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\n# print('INSTRUCT MODEL:')\n# print(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:24:08.852475Z","iopub.execute_input":"2023-12-27T14:24:08.852963Z","iopub.status.idle":"2023-12-27T14:24:09.720699Z","shell.execute_reply.started":"2023-12-27T14:24:08.852924Z","shell.execute_reply":"2023-12-27T14:24:09.719277Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2388080517387528, 'rouge2': 0.08595817328211694, 'rougeL': 0.1814362801134182, 'rougeLsum': 0.23031945093532405}\nPEFT MODEL:\n{'rouge1': 0.25536953771640963, 'rouge2': 0.10062124166103695, 'rougeL': 0.19085985743535694, 'rougeLsum': 0.21585355759610594}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice, that PEFT model results are not too bad, while the training process was much easier!","metadata":{}},{"cell_type":"markdown","source":"Calculate the improvement of PEFT over the original model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:24:12.959421Z","iopub.execute_input":"2023-12-27T14:24:12.960096Z","iopub.status.idle":"2023-12-27T14:24:12.971126Z","shell.execute_reply.started":"2023-12-27T14:24:12.960060Z","shell.execute_reply":"2023-12-27T14:24:12.969575Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 1.66%\nrouge2: 1.47%\nrougeL: 0.94%\nrougeLsum: -1.45%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Further finetuning would result better as we work on trainable parameters","metadata":{}}]}