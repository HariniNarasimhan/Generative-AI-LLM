# Generative-AI-LLM
Generative AI with LLM, From prompt engineering to Fine-tuning and Domain adaptation

The above objective is achieved with a use case as "News article summarization into 60 words - An replica of [Inshorts](https://www.inshorts.com/) using Gen AI"

The [Prompt Engineering Part 1](https://github.com/HariniNarasimhan/Generative-AI-LLM/blob/main/LLM-Prompt-Engineering-Part1.ipynb) covers the following
1. General Dataset loading from huggigface
2. Model loading from hugging face
3. Template understanding of the model's input for your use case
4. Instruction prompting
5. Zero-shot Inference
6. One-shot and Few-shot inference
7. General configuration parameters (max_new_tokens, top-k, top-p, temperature )and their impact on the model's results.

Prompt Engineering Part 2 - To be implemented


The [Fine-tuning of LLM](https://github.com/HariniNarasimhan/Generative-AI-LLM/blob/main/Fine-tuning-LLM-PEFT.ipynb) covers the following
1. Full fine tuning of LLM for custom dataset
2. Challenges in fine tuning - memory issues
3. Fine-tuning using LoRA for PEFT (Parameter Efficient Fine tuning)
4. Evaluation for summarization task using ROUGE metric
   
   
To be Continued!
